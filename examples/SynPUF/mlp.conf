# For the weight, remember to adjust to 1/n (1/3, 1/2...)
name: "deep-big-simple-mlp"
train_steps: 10000
test_steps:5
test_frequency:20
display_frequency:10
debug: true
alg: kBackPropagation
updater{
  base_lr: 0.001
  lr_change: kStep
  type: kSGD
  step_conf{
    change_freq: 60
    gamma: 0.997
  }
}

neuralnet {
layer {
  name: "data"
  type: kShardData
  sharddata_conf {
    path: "examples/NUH/Readmission_multisrc_downtrain_shard"
    batchsize: 1000
  }
  exclude: kTest
}

layer {
  name: "data"
  type: kShardData
  sharddata_conf {
    path: "examples/NUH/Readmission_multisrc_downtest_shard"
    batchsize: 1000
  }
  exclude: kTrain
}

layer{
  name:"MultisrcData"
  type: kMultiSrcData
  srclayers: "data"
  multisrcdata_conf{
    diag_dim: 12434
    proc_dim: 7093
    demo_dim: 34
  }
}

layer{
  name: "label"
  type: kLabel
  srclayers: "data"
}

layer{
  name: "fc1"
  type: kInnerProduct
  srclayers:"MultisrcData"
  innerproduct_conf{
    num_output: 2500
  }
  param{
    name: "weight"
    init_method: kUniformSqrtFanInOut
    low:-2.45
    high:2.45
  }
  param{
    name: "bias"
    init_method: kConstant
    value: 0.0
  }
}

layer{
  name: "tanh1"
  type: kTanh
  srclayers:"fc1"
}

layer{
  name: "fc2"
  type: kInnerProduct
  srclayers:"tanh1"
  innerproduct_conf{
    num_output: 2
  }
  param{
    name: "weight"
    init_method: kUniformSqrtFanInOut
    low:-2.45
    high:2.45
  }
  param{
    name: "bias"
    init_method: kConstant
    value: 0.0
  }
}

layer{
  name: "loss"
  type: kSoftmaxLoss
  softmaxloss_conf{
    topk:1
  }
  srclayers:"fc2"
  srclayers:"label"
}
}
